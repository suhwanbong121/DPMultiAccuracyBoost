{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7305c39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a2a65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (13143, 100)\n",
      "attrs shape: (13143, 73)\n",
      "Number of attributes: 73\n",
      "\n",
      "Top pairs by abs(group accuracy gap):\n",
      "Target=Wearing Lipstick   | Protected=Male         | acc=0.874 | acc_g0=0.598 | acc_g1=0.957 | gap=0.359 | n_g0=610, n_g1=2019\n",
      "Target=Heavy Makeup       | Protected=Male         | acc=0.890 | acc_g0=0.621 | acc_g1=0.971 | gap=0.350 | n_g0=607, n_g1=2022\n",
      "Target=Attractive Woman   | Protected=Male         | acc=0.867 | acc_g0=0.599 | acc_g1=0.938 | gap=0.339 | n_g0=548, n_g1=2081\n",
      "Target=Attractive Woman   | Protected=Youth        | acc=0.867 | acc_g0=0.908 | acc_g1=0.647 | gap=0.261 | n_g0=2215, n_g1=414\n",
      "Target=Attractive Man     | Protected=Senior       | acc=0.711 | acc_g0=0.662 | acc_g1=0.890 | gap=0.228 | n_g0=2073, n_g1=556\n",
      "Target=Heavy Makeup       | Protected=Youth        | acc=0.890 | acc_g0=0.929 | acc_g1=0.702 | gap=0.227 | n_g0=2182, n_g1=447\n",
      "Target=Wearing Lipstick   | Protected=Youth        | acc=0.874 | acc_g0=0.904 | acc_g1=0.718 | gap=0.186 | n_g0=2203, n_g1=426\n",
      "Target=Attractive Man     | Protected=Youth        | acc=0.711 | acc_g0=0.736 | acc_g1=0.577 | gap=0.159 | n_g0=2201, n_g1=428\n",
      "Target=Attractive Man     | Protected=Black        | acc=0.711 | acc_g0=0.701 | acc_g1=0.857 | gap=0.156 | n_g0=2475, n_g1=154\n",
      "Target=Attractive Woman   | Protected=Senior       | acc=0.867 | acc_g0=0.834 | acc_g1=0.978 | gap=0.144 | n_g0=2027, n_g1=602\n",
      "Target=Heavy Makeup       | Protected=Senior       | acc=0.890 | acc_g0=0.864 | acc_g1=0.986 | gap=0.121 | n_g0=2065, n_g1=564\n",
      "Target=Attractive Man     | Protected=Asian        | acc=0.711 | acc_g0=0.701 | acc_g1=0.822 | gap=0.121 | n_g0=2416, n_g1=213\n",
      "Target=Attractive Man     | Protected=White        | acc=0.711 | acc_g0=0.800 | acc_g1=0.680 | gap=0.120 | n_g0=675, n_g1=1954\n",
      "Target=Wearing Lipstick   | Protected=Senior       | acc=0.874 | acc_g0=0.849 | acc_g1=0.965 | gap=0.116 | n_g0=2062, n_g1=567\n",
      "Target=Wearing Lipstick   | Protected=Black        | acc=0.874 | acc_g0=0.868 | acc_g1=0.960 | gap=0.092 | n_g0=2454, n_g1=175\n",
      "Target=Attractive Man     | Protected=Male         | acc=0.711 | acc_g0=0.643 | acc_g1=0.731 | gap=0.088 | n_g0=622, n_g1=2007\n",
      "Target=Wearing Lipstick   | Protected=Asian        | acc=0.874 | acc_g0=0.880 | acc_g1=0.805 | gap=0.076 | n_g0=2414, n_g1=215\n",
      "Target=Frowning           | Protected=Indian       | acc=0.713 | acc_g0=0.714 | acc_g1=0.643 | gap=0.071 | n_g0=2573, n_g1=56\n",
      "Target=Attractive Man     | Protected=Indian       | acc=0.711 | acc_g0=0.712 | acc_g1=0.642 | gap=0.070 | n_g0=2576, n_g1=53\n",
      "Target=Smiling            | Protected=Indian       | acc=0.704 | acc_g0=0.705 | acc_g1=0.652 | gap=0.053 | n_g0=2560, n_g1=69\n",
      "Target=Attractive Woman   | Protected=Indian       | acc=0.867 | acc_g0=0.866 | acc_g1=0.914 | gap=0.048 | n_g0=2571, n_g1=58\n",
      "Target=Heavy Makeup       | Protected=Indian       | acc=0.890 | acc_g0=0.892 | acc_g1=0.851 | gap=0.040 | n_g0=2555, n_g1=74\n",
      "Target=Smiling            | Protected=Black        | acc=0.704 | acc_g0=0.706 | acc_g1=0.669 | gap=0.038 | n_g0=2457, n_g1=172\n",
      "Target=Wearing Lipstick   | Protected=Indian       | acc=0.874 | acc_g0=0.873 | acc_g1=0.907 | gap=0.034 | n_g0=2575, n_g1=54\n",
      "Target=Attractive Woman   | Protected=Asian        | acc=0.867 | acc_g0=0.865 | acc_g1=0.894 | gap=0.029 | n_g0=2422, n_g1=207\n",
      "Target=Smiling            | Protected=Senior       | acc=0.704 | acc_g0=0.709 | acc_g1=0.684 | gap=0.024 | n_g0=2081, n_g1=548\n",
      "Target=Frowning           | Protected=Black        | acc=0.713 | acc_g0=0.711 | acc_g1=0.735 | gap=0.024 | n_g0=2474, n_g1=155\n",
      "Target=Heavy Makeup       | Protected=Black        | acc=0.890 | acc_g0=0.889 | acc_g1=0.912 | gap=0.023 | n_g0=2447, n_g1=182\n",
      "Target=Smiling            | Protected=Youth        | acc=0.704 | acc_g0=0.707 | acc_g1=0.685 | gap=0.022 | n_g0=2213, n_g1=416\n",
      "Target=Frowning           | Protected=Youth        | acc=0.713 | acc_g0=0.709 | acc_g1=0.730 | gap=0.021 | n_g0=2206, n_g1=423\n",
      "Target=Frowning           | Protected=Senior       | acc=0.713 | acc_g0=0.717 | acc_g1=0.698 | gap=0.019 | n_g0=2063, n_g1=566\n",
      "Target=Attractive Woman   | Protected=Black        | acc=0.867 | acc_g0=0.866 | acc_g1=0.884 | gap=0.018 | n_g0=2448, n_g1=181\n",
      "Target=Smiling            | Protected=Asian        | acc=0.704 | acc_g0=0.705 | acc_g1=0.687 | gap=0.018 | n_g0=2418, n_g1=211\n",
      "Target=Heavy Makeup       | Protected=Asian        | acc=0.890 | acc_g0=0.889 | acc_g1=0.907 | gap=0.018 | n_g0=2446, n_g1=183\n",
      "Target=Frowning           | Protected=White        | acc=0.713 | acc_g0=0.725 | acc_g1=0.709 | gap=0.016 | n_g0=629, n_g1=2000\n",
      "Target=Smiling            | Protected=White        | acc=0.704 | acc_g0=0.696 | acc_g1=0.706 | gap=0.011 | n_g0=667, n_g1=1962\n",
      "Target=Heavy Makeup       | Protected=White        | acc=0.890 | acc_g0=0.898 | acc_g1=0.888 | gap=0.010 | n_g0=667, n_g1=1962\n",
      "Target=Smiling            | Protected=Male         | acc=0.704 | acc_g0=0.696 | acc_g1=0.706 | gap=0.010 | n_g0=602, n_g1=2027\n",
      "Target=Frowning           | Protected=Male         | acc=0.713 | acc_g0=0.720 | acc_g1=0.711 | gap=0.009 | n_g0=593, n_g1=2036\n",
      "Target=Wearing Lipstick   | Protected=White        | acc=0.874 | acc_g0=0.878 | acc_g1=0.873 | gap=0.005 | n_g0=670, n_g1=1959\n",
      "Target=Frowning           | Protected=Asian        | acc=0.713 | acc_g0=0.713 | acc_g1=0.710 | gap=0.003 | n_g0=2446, n_g1=183\n",
      "Target=Attractive Woman   | Protected=White        | acc=0.867 | acc_g0=0.868 | acc_g1=0.867 | gap=0.001 | n_g0=683, n_g1=1946\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "with open(\"dataset_description.pkl\", \"rb\") as f:\n",
    "    dc = pickle.load(f)\n",
    "\n",
    "X = dc[\"latent_vars\"]      # shape (n, 100)\n",
    "attrs = dc[\"attributes\"]   # shape (n, 73)\n",
    "fields = dc[\"fields\"]      # list of 73 attribute names\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"attrs shape:\", attrs.shape)\n",
    "print(\"Number of attributes:\", len(fields))\n",
    "\n",
    "def bin_attr(attrs, idx):\n",
    "    \"\"\"\n",
    "    Convert attribute column to binary {0,1} assuming signed attributes.\n",
    "    Positive -> 1, non-positive -> 0.\n",
    "    \"\"\"\n",
    "    return (attrs[:, idx] > 0).astype(int)\n",
    "\n",
    "def eval_pair(target_name, protected_name, test_size=0.4, random_state=0):\n",
    "    \"\"\"\n",
    "    Train a logistic regression classifier for target_name and\n",
    "    evaluate accuracy and group-wise accuracy by protected_name.\n",
    "    Returns a dict with metrics.\n",
    "    \"\"\"\n",
    "    if target_name not in fields or protected_name not in fields:\n",
    "        raise ValueError(\"Attribute name not found in fields list.\")\n",
    "    \n",
    "    j_y = fields.index(target_name)\n",
    "    j_g = fields.index(protected_name)\n",
    "    \n",
    "    y = bin_attr(attrs, j_y)\n",
    "    g = bin_attr(attrs, j_g)\n",
    "    \n",
    "    # Split into train / (audit + test)\n",
    "    X_train, X_tmp, y_train, y_tmp, g_train, g_tmp = train_test_split(\n",
    "        X, y, g, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Split (audit + test) into audit and test (50/50)\n",
    "    X_audit, X_test, y_audit, y_test, g_audit, g_test = train_test_split(\n",
    "        X_tmp, y_tmp, g_tmp, test_size=0.5, random_state=random_state + 1, stratify=y_tmp\n",
    "    )\n",
    "    \n",
    "    # Base logistic regression\n",
    "    base = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "    base.fit(X_train, y_train)\n",
    "    p_test = base.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (p_test >= 0.5).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Group-wise accuracy\n",
    "    acc_groups = {}\n",
    "    sizes = {}\n",
    "    for val in [0, 1]:\n",
    "        mask = (g_test == val)\n",
    "        sizes[val] = int(mask.sum())\n",
    "        if mask.sum() == 0:\n",
    "            acc_groups[val] = np.nan\n",
    "        else:\n",
    "            acc_groups[val] = accuracy_score(y_test[mask], y_pred[mask])\n",
    "    \n",
    "    if np.all(np.isfinite(list(acc_groups.values()))):\n",
    "        gap = abs(acc_groups[0] - acc_groups[1])\n",
    "    else:\n",
    "        gap = np.nan\n",
    "    \n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"acc_g0\": acc_groups[0],\n",
    "        \"acc_g1\": acc_groups[1],\n",
    "        \"gap\": gap,\n",
    "        \"n_test\": len(y_test),\n",
    "        \"n_g0\": sizes[0],\n",
    "        \"n_g1\": sizes[1],\n",
    "    }\n",
    "\n",
    "# Candidate targets and protected attributes we want to scan\n",
    "candidate_targets = [\n",
    "    \"Smiling\",\n",
    "    \"Frowning\",\n",
    "    \"Attractive Man\",\n",
    "    \"Attractive Woman\",\n",
    "    \"Heavy Makeup\",\n",
    "    \"Wearing Lipstick\",\n",
    "]\n",
    "\n",
    "candidate_protected = [\n",
    "    \"Male\",\n",
    "    \"White\",\n",
    "    \"Black\",\n",
    "    \"Asian\",\n",
    "    \"Indian\",\n",
    "    \"Youth\",\n",
    "    \"Senior\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for t_name in candidate_targets:\n",
    "    if t_name not in fields:\n",
    "        continue\n",
    "    for g_name in candidate_protected:\n",
    "        if g_name not in fields:\n",
    "            continue\n",
    "        if t_name == g_name:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            stats = eval_pair(t_name, g_name)\n",
    "            results.append((t_name, g_name, stats))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping pair ({t_name}, {g_name}) due to error: {e}\")\n",
    "\n",
    "# Sort by absolute group accuracy gap (largest first)\n",
    "results_sorted = sorted(results, key=lambda x: -x[2][\"gap\"])\n",
    "\n",
    "print(\"\\nTop pairs by abs(group accuracy gap):\")\n",
    "for (t_name, g_name, s) in results_sorted:\n",
    "    print(\n",
    "        f\"Target={t_name:18s} | Protected={g_name:12s} | \"\n",
    "        f\"acc={s['acc']:.3f} | acc_g0={s['acc_g0']:.3f} | \"\n",
    "        f\"acc_g1={s['acc_g1']:.3f} | gap={s['gap']:.3f} | \"\n",
    "        f\"n_g0={s['n_g0']}, n_g1={s['n_g1']}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abaf9420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "0 Male\n",
      "1 Asian\n",
      "2 White\n",
      "3 Black\n",
      "4 Baby\n",
      "5 Child\n",
      "6 Youth\n",
      "7 Middle Aged\n",
      "8 Senior\n",
      "9 Black Hair\n",
      "10 Blond Hair\n",
      "11 Brown Hair\n",
      "12 Bald\n",
      "13 No Eyewear\n",
      "14 Eyeglasses\n",
      "15 Sunglasses\n",
      "16 Mustache\n",
      "17 Smiling\n",
      "18 Frowning\n",
      "19 Chubby\n",
      "20 Blurry\n",
      "21 Harsh Lighting\n",
      "22 Flash\n",
      "23 Soft Lighting\n",
      "24 Outdoor\n",
      "25 Curly Hair\n",
      "26 Wavy Hair\n",
      "27 Straight Hair\n",
      "28 Receding Hairline\n",
      "29 Bangs\n",
      "30 Sideburns\n",
      "31 Fully Visible Forehead\n",
      "32 Partially Visible Forehead\n",
      "33 Obstructed Forehead\n",
      "34 Bushy Eyebrows\n",
      "35 Arched Eyebrows\n",
      "36 Narrow Eyes\n",
      "37 Eyes Open\n",
      "38 Big Nose\n",
      "39 Pointy Nose\n",
      "40 Big Lips\n",
      "41 Mouth Closed\n",
      "42 Mouth Slightly Open\n",
      "43 Mouth Wide Open\n",
      "44 Teeth Not Visible\n",
      "45 No Beard\n",
      "46 Goatee\n",
      "47 Round Jaw\n",
      "48 Double Chin\n",
      "49 Wearing Hat\n",
      "50 Oval Face\n",
      "51 Square Face\n",
      "52 Round Face\n",
      "53 Color Photo\n",
      "54 Posed Photo\n",
      "55 Attractive Man\n",
      "56 Attractive Woman\n",
      "57 Indian\n",
      "58 Gray Hair\n",
      "59 Bags Under Eyes\n",
      "60 Heavy Makeup\n",
      "61 Rosy Cheeks\n",
      "62 Shiny Skin\n",
      "63 Pale Skin\n",
      "64 5 o Clock Shadow\n",
      "65 Strong Nose-Mouth Lines\n",
      "66 Wearing Lipstick\n",
      "67 Flushed Face\n",
      "68 High Cheekbones\n",
      "69 Brown Eyes\n",
      "70 Wearing Earrings\n",
      "71 Wearing Necktie\n",
      "72 Wearing Necklace\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset_description.pkl\", \"rb\") as f:\n",
    "    dc = pickle.load(f)\n",
    "\n",
    "fields = dc[\"fields\"]\n",
    "print(len(fields))\n",
    "for i, name in enumerate(fields):\n",
    "    print(i, name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8d8816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset_description.pkl\n",
      "X shape: (13143, 100)\n",
      "attrs shape: (13143, 73)\n",
      "Number of attributes: 73\n",
      "\n",
      "Target attribute = Frowning\n",
      "Protected attribute = Indian\n",
      "Positive rate (y=1): 0.581\n",
      "Group 1 fraction (g=1): 0.023\n",
      "\n",
      "Dataset sizes:\n",
      "  Train:  7885\n",
      "  Audit:  2629\n",
      "  Test  :  2629\n",
      "=== Base model ===\n",
      "Accuracy: 0.713\n",
      "  Group 0 accuracy (2573 samples): 0.714\n",
      "  Group 1 accuracy (56 samples): 0.643\n",
      "  |accuracy gap| (group 0 vs 1): 0.071\n",
      "\n",
      "[Non-DP MA] Round 0: correlation delta_t = 0.010194\n",
      "[Non-DP MA] Round 1: correlation delta_t = 0.009435\n",
      "[Non-DP MA] Round 2: correlation delta_t = 0.008732\n",
      "[Non-DP MA] Round 3: correlation delta_t = 0.008081\n",
      "[Non-DP MA] Round 4: correlation delta_t = 0.007479\n",
      "=== Non-DP multiaccuracy (post-processed) ===\n",
      "Accuracy: 0.711\n",
      "  Group 0 accuracy (2573 samples): 0.713\n",
      "  Group 1 accuracy (56 samples): 0.625\n",
      "  |accuracy gap| (group 0 vs 1): 0.088\n",
      "\n",
      "Exact correlations per round (non-DP): [0.010194279930917071, 0.009435071570672604, 0.008731967457888604, 0.008081043025248, 0.007478601964363654]\n",
      "\n",
      "[DP MA] Round 0: noisy correlation delta_hat = 0.060551\n",
      "[DP MA] Round 1: noisy correlation delta_hat = -0.083309\n",
      "[DP MA] Round 2: noisy correlation delta_hat = -0.010056\n",
      "[DP MA] Round 3: noisy correlation delta_hat = -0.049808\n",
      "[DP MA] Round 4: noisy correlation delta_hat = 0.045866\n",
      "=== DP multiaccuracy (post-processed) ===\n",
      "Accuracy: 0.601\n",
      "  Group 0 accuracy (2573 samples): 0.600\n",
      "  Group 1 accuracy (56 samples): 0.643\n",
      "  |accuracy gap| (group 0 vs 1): 0.042\n",
      "\n",
      "Noisy correlation estimates per round (DP): [0.060551434240985, -0.08330887203862025, -0.010056265584927202, -0.049807591155643115, 0.04586647283816526]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Load dataset and choose target / protected pair\n",
    "# ---------------------------------------------------\n",
    "\n",
    "with open(\"dataset_description.pkl\", \"rb\") as f:\n",
    "    dc = pickle.load(f)\n",
    "\n",
    "X = dc[\"latent_vars\"]        # shape (N, d)\n",
    "attrs = dc[\"attributes\"]     # shape (N, 73)\n",
    "fields = dc[\"fields\"]        # list of 73 attribute names\n",
    "\n",
    "print(\"Loaded dataset_description.pkl\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"attrs shape:\", attrs.shape)\n",
    "print(\"Number of attributes:\", len(fields))\n",
    "\n",
    "# Choose the target and protected attributes\n",
    "target_name = \"Frowning\"   # label to predict\n",
    "protected_name = \"Indian\"            # protected attribute\n",
    "\n",
    "j_y = fields.index(target_name)\n",
    "j_g = fields.index(protected_name)\n",
    "\n",
    "# Binary label and protected group (attrs > 0 means attribute present)\n",
    "y = (attrs[:, j_y] > 0).astype(int)\n",
    "g = (attrs[:, j_g] > 0).astype(int)\n",
    "\n",
    "print(f\"\\nTarget attribute = {target_name}\")\n",
    "print(f\"Protected attribute = {protected_name}\")\n",
    "print(\"Positive rate (y=1): {:.3f}\".format(y.mean()))\n",
    "print(\"Group 1 fraction (g=1): {:.3f}\".format(g.mean()))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Train / Audit / Test split\n",
    "# ---------------------------------------------------\n",
    "\n",
    "X_train, X_tmp, y_train, y_tmp, g_train, g_tmp = train_test_split(\n",
    "    X, y, g, test_size=0.4, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "X_audit, X_test, y_audit, y_test, g_audit, g_test = train_test_split(\n",
    "    X_tmp, y_tmp, g_tmp, test_size=0.5, random_state=1, stratify=y_tmp\n",
    ")\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(\"  Train: \", X_train.shape[0])\n",
    "print(\"  Audit: \", X_audit.shape[0])\n",
    "print(\"  Test  : \", X_test.shape[0])\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Helper: metrics and multiaccuracy update\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def report_metrics(name, y_true, p, g=None):\n",
    "    \"\"\"\n",
    "    Print overall accuracy and (optionally) per-group accuracies.\n",
    "    \n",
    "    Args:\n",
    "        name: String name for the model\n",
    "        y_true: True labels (0/1), shape (n,)\n",
    "        p: Predicted probabilities in [0,1], shape (n,)\n",
    "        g: Protected group labels (0/1), shape (n,) or None\n",
    "    \"\"\"\n",
    "    y_pred = (p >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    \n",
    "    if g is not None:\n",
    "        accs = {}\n",
    "        for val in [0, 1]:\n",
    "            mask = (g == val)\n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            acc_g = accuracy_score(y_true[mask], y_pred[mask])\n",
    "            accs[val] = acc_g\n",
    "            print(f\"  Group {val} accuracy ({mask.sum()} samples): {acc_g:.3f}\")\n",
    "        if 0 in accs and 1 in accs:\n",
    "            gap = abs(accs[0] - accs[1])\n",
    "            print(f\"  |accuracy gap| (group 0 vs 1): {gap:.3f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def multiaccuracy_update(probs, h_vals, eta):\n",
    "    \"\"\"\n",
    "    Multiaccuracy multiplicative-weights-style update on probabilities.\n",
    "    \n",
    "    We interpret probs as sigmoid(logits). We move logits in direction -eta * h(x)\n",
    "    and then map back through sigmoid.\n",
    "    \n",
    "    Args:\n",
    "        probs: Current probabilities in [0,1], shape (n,)\n",
    "        h_vals: Auditor outputs h_t(x), shape (n,)\n",
    "        eta: Learning rate for the update\n",
    "        \n",
    "    Returns:\n",
    "        Updated probabilities in [0,1], shape (n,)\n",
    "    \"\"\"\n",
    "    # Avoid numerical issues at 0 and 1\n",
    "    eps = 1e-6\n",
    "    p = np.clip(probs, eps, 1 - eps)\n",
    "    \n",
    "    # logit(p) = log(p / (1-p))\n",
    "    logits = np.log(p / (1.0 - p))\n",
    "    \n",
    "    # Update logits by moving in the opposite direction of auditor prediction\n",
    "    new_logits = logits - eta * h_vals\n",
    "    \n",
    "    # Map back through sigmoid\n",
    "    new_probs = 1.0 / (1.0 + np.exp(-new_logits))\n",
    "    return new_probs\n",
    "\n",
    "\n",
    "def run_non_dp_multiaccuracy_boost(\n",
    "    X_train, y_train,\n",
    "    X_audit, y_audit,\n",
    "    X_test, y_test,\n",
    "    base_model,\n",
    "    T=5,\n",
    "    eta=0.5,\n",
    "    auditor_lr=0.05,\n",
    "    auditor_steps=200,\n",
    "    stop_threshold=0.002,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Non-DP multiaccuracy boosting (no privacy, exact correlation).\n",
    "    \n",
    "    We:\n",
    "      1) Start from base_model probabilities on audit and test.\n",
    "      2) At each round t:\n",
    "         - Fit a linear regression auditor to residuals on the audit set.\n",
    "         - Compute exact correlation delta_t = E[h_t(x)*(f_t(x)-y)] on audit.\n",
    "         - If |delta_t| < stop_threshold, stop.\n",
    "         - Otherwise update f_t on both audit and test using multiaccuracy_update.\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: training data (not directly used here, only for base_model fitting outside)\n",
    "        X_audit, y_audit: audit set\n",
    "        X_test, y_test: test set (for evaluation of post-processed classifier)\n",
    "        base_model: already-fitted probabilistic classifier with predict_proba\n",
    "        T: maximum number of boosting rounds\n",
    "        eta: multiaccuracy learning rate\n",
    "        auditor_lr: (unused here; kept to mirror DP version signature)\n",
    "        auditor_steps: (unused here)\n",
    "        stop_threshold: stop if |delta_t| < this value\n",
    "        seed: random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        dict with:\n",
    "            \"fT_audit\": final probabilities on audit\n",
    "            \"fT_test\" : final probabilities on test\n",
    "            \"history\" : dict with \"delta\" (exact correlations per round)\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Start from base-model probabilities\n",
    "    f_audit = base_model.predict_proba(X_audit)[:, 1]\n",
    "    f_test = base_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    deltas = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Compute residuals on audit\n",
    "        residuals = f_audit - y_audit  # shape (n_audit,)\n",
    "        \n",
    "        # Fit a linear regression auditor h_t(x) â‰ˆ residual\n",
    "        # We use a simple closed-form / sklearn LinearRegression (non-DP)\n",
    "        auditor = LinearRegression()\n",
    "        auditor.fit(X_audit, residuals)\n",
    "        \n",
    "        # Auditor predictions on audit and test\n",
    "        h_audit = auditor.predict(X_audit)\n",
    "        h_test = auditor.predict(X_test)\n",
    "        \n",
    "        # Exact correlation on the audit set\n",
    "        delta_t = np.mean(h_audit * (f_audit - y_audit))\n",
    "        deltas.append(delta_t)\n",
    "        \n",
    "        print(f\"[Non-DP MA] Round {t}: correlation delta_t = {delta_t:.6f}\")\n",
    "        \n",
    "        # Stopping condition\n",
    "        if abs(delta_t) < stop_threshold:\n",
    "            print(f\"Stopping early at round {t} (|delta_t| < {stop_threshold}).\")\n",
    "            break\n",
    "        \n",
    "        # Multiaccuracy update on both audit and test probabilities\n",
    "        f_audit = multiaccuracy_update(f_audit, h_audit, eta)\n",
    "        f_test = multiaccuracy_update(f_test, h_test, eta)\n",
    "    \n",
    "    return {\n",
    "        \"fT_audit\": f_audit,\n",
    "        \"fT_test\": f_test,\n",
    "        \"history\": {\n",
    "            \"delta\": deltas,\n",
    "        },\n",
    "    }\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Base model (no multiaccuracy, no DP)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "base = LogisticRegression(max_iter=1000, solver=\"lbfgs\")\n",
    "base.fit(X_train, y_train)\n",
    "\n",
    "p_test_base = base.predict_proba(X_test)[:, 1]\n",
    "report_metrics(\"Base model\", y_test, p_test_base, g_test)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Non-DP multiaccuracy boosting\n",
    "# ---------------------------------------------------\n",
    "\n",
    "T = 5               # max number of boosting rounds\n",
    "eta = 0.2           # multiaccuracy learning rate\n",
    "stop_threshold = 0.002\n",
    "\n",
    "results_ma = run_non_dp_multiaccuracy_boost(\n",
    "    X_train, y_train,\n",
    "    X_audit, y_audit,\n",
    "    X_test, y_test,\n",
    "    base_model=base,\n",
    "    T=T,\n",
    "    eta=eta,\n",
    "    auditor_lr=0.05,       # kept for symmetry with DP version\n",
    "    auditor_steps=200,\n",
    "    stop_threshold=stop_threshold,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "p_test_ma = results_ma[\"fT_test\"]\n",
    "report_metrics(\"Non-DP multiaccuracy (post-processed)\", y_test, p_test_ma, g_test)\n",
    "\n",
    "print(\"Exact correlations per round (non-DP):\", results_ma[\"history\"][\"delta\"])\n",
    "print()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. DP multiaccuracy boosting (using your DP code)\n",
    "# ---------------------------------------------------\n",
    "from dp_multiaccuracy_utils import run_dp_multiaccuracy_boost\n",
    "\n",
    "\n",
    "epsilon_round = 1      # per-round epsilon\n",
    "delta_round   = 1     # per-round delta\n",
    "\n",
    "T = 5\n",
    "eta = 0.2\n",
    "\n",
    "results_dp = run_dp_multiaccuracy_boost(\n",
    "    X_train, y_train,\n",
    "    X_audit, y_audit,\n",
    "    X_test, y_test,\n",
    "    base_model=base,\n",
    "    T=T,\n",
    "    eta=eta,\n",
    "    epsilon_round=epsilon_round,\n",
    "    delta_round=delta_round,\n",
    "    clipping_grad=1.0,\n",
    "    clipping_corr=1.0,\n",
    "    auditor_lr=0.05,\n",
    "    auditor_steps=200,\n",
    "    auditor_batch_size=256,\n",
    "    stop_threshold=0.002,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "p_test_dp = results_dp[\"fT_test\"]\n",
    "report_metrics(\"DP multiaccuracy (post-processed)\", y_test, p_test_dp, g_test)\n",
    "\n",
    "print(\"Noisy correlation estimates per round (DP):\", results_dp[\"history\"][\"delta_hat\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d75c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
